import numpy
import _pickle as cpickle
import gzip
import random

"""
Implementation of a triple layered feed forward neural network that
uses a stochastic gradient descent to learn the pattern in data.
"""


class Recognizer:
    def __init__(self, layers):
        """
        Initializing the 3 layer, neural network.
        :param layers:  list including the number of neurons
                        in different layers of the neural network.
        """
        self.totalLayers = len(layers)
        self.layers = layers

        # Randomly generate the biases and weights for input neurons. The random function generates data from a set with
        # mean 0 and std dev 1.
        self.biases = [numpy.random.randn(y, 1) for y in layers[1:]]
        self.weights = [numpy.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]

    def backpropagate(self, ip, op):
        """
        Performs the correction on the weights and biases based on the output of the cost function
        :param ip: A single input (28 x 28 pixel data)
        :param op: The expected output value
        :return:
        """
        # Stores all sigmoid values generated by each input in ip
        output = []
        act = ip
        # list to store all activations layer by layer
        acts = [ip]
        new_bias = [numpy.zeros(bias.shape) for bias in self.biases]
        new_weight = [numpy.zeros(wt.shape) for wt in self.weights]

        # Calculating the activations for the given input
        for bias, wt in zip(self.biases, self.weights):
            # Calculating w * input + bias for input layer
            z = numpy.dot(wt, act) + bias
            output.append(z)
            # Calculating the sigmoid of z
            act = self.sigmoid(z)
            # and storing the output activation in acts
            acts.append(act)
        # Performing backpropagation,
        # starting with the output layer
        # partial derivation of the cost function wrt activation * derivative of the sigmoid function
        d = (acts[-1] - op)

        # Using the obtained delta to further propagate to previous layers
        new_bias[-1] = d
        # Working with the layers before output layer. acts[-2] refers to the layer
        # just before the output layer
        new_weight[-1] = numpy.dot(d, acts[-2].transpose())

        # Propagating through remaining layers
        for i in range(2, self.totalLayers):
            # output[-i] is the set of calculated sigmoid values calculated above
            z = output[-i]

            # Calculating the derivative of the sigmoid values
            sp = self.sigmoid_prime(z)

            # Calculating the delta to be propagated further
            d = numpy.dot(self.weights[-i + 1].transpose(), d) * sp

            # Storing the new bias values for this layer
            new_bias[-i] = d
            # Storing the corrected weights for this layer
            new_weight[-i] = numpy.dot(d, acts[-i - 1].transpose())
        return new_bias, new_weight

    def stochasticGradientDescent(self, trainSet, testSet, tries, learningRate, lmda, batchSize):
        """
        Using Stochastic gradient descent to obtain the best weights and biases for the given dataset
        :param trainSet: training dataset
        :param testSet: test dataset
        :param tries: number of iterations or 'epochs' to train the network
        :param learningRate: The rate of learning
        :param lmda: Regularization parameter
        :param batchSize: The size of each mini batch
        :return: None
        """

        testSet = list(testSet)
        ntest = len(testSet)
        trainSet = list(trainSet)
        ntrain = len(trainSet)

        # Running for each epoch now.
        for j in range(tries):
            # Shuffling the training data to create randomness in batches for every iteration
            random.shuffle(trainSet)
            # Dividing training set into batches of batchSize
            batches = []
            for k in range(0, ntrain, batchSize):
                batches.append(trainSet[k: k+batchSize])

            for batch in batches:

                # Creating empty matrices to store the new bias and weight values
                new_bias = [numpy.zeros(b.shape) for b in self.biases]
                new_weight = [numpy.zeros(w.shape) for w in self.weights]

                for input, output in batch:
                    # Calculating the change in weights and biases using backpropagation
                    d_new_bias, d_new_weight = self.backpropagate(input, output)

                    # Adding the change to each bias and weight over iterations.
                    new_bias = [nbias + dnbias for nbias, dnbias in zip(new_bias, d_new_bias)]
                    new_weight = [nweight + dnweight for nweight, dnweight in zip(new_weight, d_new_weight)]

                # Correcting the bias and weights according to obtained results.
                self.biases = [bias - (learningRate / len(batch)) * nbias
                               for bias, nbias in zip(self.biases, new_bias)]
                # Introducing the lambda regularization constant here
                self.weights = [(1 - learningRate*(lmda/len(trainSet)))*wgt - (learningRate / len(batch)) * nweight
                                for wgt, nweight in zip(self.weights, new_weight)]
            print("Test " + str(j) + ": " + str(self.testNetwork(testSet) * 100 / ntest) + "% accuracy!")

    def testNetwork(self, data):
        """
        Uses test data and compares the output obtained with the expected output. 
        Returns the total correct classifications made
        :param data: Test data containing input and the expected output
        :return: Total number of test samples matched
        """

        correctTotal = 0
        for (input, expectedOutput) in data:
            networkResult = numpy.argmax(self.networkOutput(input))
            if networkResult == expectedOutput:
                correctTotal = correctTotal + 1
        return correctTotal

        #
        #     results = [
        #             (numpy.argmax(self.networkOutput(input)), expectedOutput)
        #                 for (input, expectedOutput) in data
        #           ]
        # return sum(int(i == j) for (i, j) in results)
        #
        #

    def networkOutput(self, input):
        """
        process the test dataset to get result and
        compare it with the expected output.
        :param input: array of pixel information
        :return: sigmoid conversion of the input
        """
        for bias, weight in zip(self.biases, self.weights):
            # The expression used here is the sigmoid function.
            input = self.sigmoid(numpy.dot(weight, input) + bias)
        # Returning the result generated by the neural network on the given input
        return input

    def sigmoid(self, z):
        """The sigmoid function."""
        return 1.0 / (1.0 + numpy.exp(-z))

    def sigmoid_prime(self, z):
        """Derivative of the sigmoid function."""
        return self.sigmoid(z) * (1 - self.sigmoid(z))

class DataLoader:

    def loadData(self):
        """
        The dataset is in the form of a gzipped pickle file, first unpacking 
        the pickle dump and then loading the individual components using pickle.
        :return: Training and testing dataset
        """

        # Unpacking the dataset
        file = gzip.open('mnist.pkl.gz', 'rb')

        # Extracting the data
        train, val, test = cpickle.load(file, encoding='latin1')
        file.close()

        # Vectorizing the pixel data to a 784 x 1 matrix
        trainInputs = [numpy.reshape(x, (784, 1)) for x in train[0]]
        # Vectorizing the expected output value to a 10 x 1 matrix
        trainResults = [self.vectorizeOutput(y) for y in train[1]]
        # Combining the inputs and outputs together
        trainData = zip(trainInputs, trainResults)

        # Vectorizing only the pixel input data. Output data doesnt require processing.
        testInputs = [numpy.reshape(x, (784, 1)) for x in test[0]]
        testData = zip(testInputs, test[1])

        return trainData, testData

    def vectorizeOutput(self, index):
        """
        Vectorizes the the expected output from the dataset into a 10 x 1 matrix
        with all values 0 and just the jth value as 1.0.
        :param index: index of the digit
        :return: 10x1 matrix
        """
        matrix = numpy.zeros((10, 1))
        matrix[index] = 1.0
        return matrix

def main():
    """
    main function. Program execution begins here.
    :return: None
    """
    # Create and initialize the reader object.
    loader = DataLoader()

    # Hyper parameters control panel
    HL_count = 100
    learning_rate = 0.1
    lmda = 5.0
    batchSize = 10

    # Create and initialize the neural network object.
    # 784 Neurons in layer 1 for 28x28 pixel inputs
    # HL_count Neurons in the middle layer i.e. the hidden layer
    # 10 Neurons in the last layer for 10 possible digits
    handwritingRecognizer = Recognizer([784, HL_count, 10])

    # Get training and test dataset from the bundled dataset
    trainData, testData = loader.loadData()

    # Executing stochastic gradient descent on the training data set(trainData)
    # 30 is the number of iterations/epochs and 3.5 is the learning rate
    # After every iteration, the network will be tested against 'testData',
    # displaying its accuracy over time. lmda is the lambda value aka the regularization constant
    print("HL_count:", str(HL_count), "\tLearning rate: ", learning_rate)
    handwritingRecognizer.stochasticGradientDescent(trainData, testData, 45, learning_rate, lmda, batchSize)

if __name__ == '__main__':
    main()
